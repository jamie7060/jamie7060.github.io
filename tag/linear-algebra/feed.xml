<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://soonjune.github.io/tag/linear-algebra/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://soonjune.github.io/" rel="alternate" type="text/html" />
  <updated>2021-01-10T14:54:49+09:00</updated>
  <id>https://soonjune.github.io/tag/linear-algebra/feed.xml</id>

  
  
  

  
    <title type="html">Seung Joon’s Blog | </title>
  

  
    <subtitle>Personal Blog of Park Seung Joon</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Matrix Decompositions</title>
      <link href="https://soonjune.github.io/matrix-decompositions" rel="alternate" type="text/html" title="Matrix Decompositions" />
      <published>2021-01-08T20:00:00+09:00</published>
      <updated>2021-01-08T20:00:00+09:00</updated>
      <id>https://soonjune.github.io/matrix-decompositions</id>
      <content type="html" xml:base="https://soonjune.github.io/matrix-decompositions">&lt;p&gt;데이터사이언스대학원 Bootcamp 4일차 수업은 4장 Matrix Decomposition을 공부했다. 선형대수의 핵심적 개념인 &lt;a href=&quot;https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#:~:text=In%20linear%20algebra%2C%20an%20eigenvector,which%20the%20eigenvector%20is%20scaled.&quot;&gt;eigenvalue와 eigenvector&lt;/a&gt;가 나오므로 잘 알아두는 게 중요하다.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;h3 id=&quot;41-determinant-and-trace&quot;&gt;4.1 Determinant and Trace&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Determinant&quot;&gt;Determinant&lt;/a&gt;는 square matrix에 대해서만 정의되며 &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplace_expansion&quot;&gt;Laplace Expansion&lt;/a&gt;을 통해 구할 수 있다. 역행렬의 존재여부를 확인하고 구하거나 n-dimensional 평행육면체의 부피를 구하는 등에 쓰인다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Trace_(linear_algebra)&quot;&gt;Trace&lt;/a&gt;는 diagonal elements의 합이다. 두 행렬이 각각 n X k, k X n이라면 교환법칙이 성립하기 때문에 cyclic permutation에 대해 invariant하다.&lt;/p&gt;

&lt;h3 id=&quot;42-eigenvalues-and-eigenvectors&quot;&gt;4.2 Eigenvalues and Eigenvectors&lt;/h3&gt;
&lt;p&gt;행렬 A의 eigenvalue와 eigenvector를 구하는 방법을 알아보자. 우선 &lt;a href=&quot;https://en.wikipedia.org/wiki/Characteristic_polynomial&quot;&gt;charateristic polynomial&lt;/a&gt;을 solve해서 λ를 구한다. 이때 구해지는 λ가 eigenvalue들이 된다. 특정 λ에 대해 (A-λI)x = O를 풀어서 구한 벡터 x들이 eigenvectors가 되며 이들의 span이 eigenspace가 된다.&lt;/p&gt;

&lt;h3 id=&quot;44-eigendecomposition-and-diagonalization&quot;&gt;4.4 Eigendecomposition and Diagonalization&lt;/h3&gt;
&lt;p&gt;Diagonal matrix D는 &lt;script type=&quot;math/tex&quot;&gt;D = P^{-1}AP&lt;/script&gt;과 같이 invertible matrix P를 이용해서 나타낼수 있는 행렬을 의미한다. 이때, A와 D는 닮음 관계에 있다고 한다. 식을 다시 표현하면 AP = PD로 나타낼 수 있는데 이는 eignevalue와 eigenvector를 정의할 때 보았던 식과 유사하다. 즉, 우리는 A를 &lt;script type=&quot;math/tex&quot;&gt;PDP^{-1}&lt;/script&gt; 꼴로 나타낼 수 있는데 이를 eigendecomposition이라고 한다. D는 entry(대각선 값)들이 eigenvalue이고 P는 그에 상응하는 eigenvector들이 column vector인 행렬이다. P는 full rank인 square matrix여야 한다. 이는 eigenvector들이 모두 선형독립이라는 말과 동치이다.&lt;/p&gt;

&lt;h3 id=&quot;45-singular-value-decomposition&quot;&gt;4.5 &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;Singular Value Decomposition&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;square matrix가 아닌 행렬도 분해해서 차원을 축소해서 나타낼수 있다. Eigendecompostion의 일반화된 버전으로 볼 수 있기에 SVD는 특정 행렬에 대해 항상 가능하다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Park Seung Joon</name>
        
        
      </author>

      

      
        <category term="linear-algebra" />
      
        <category term="math" />
      

      
        <summary type="html">데이터사이언스대학원 Bootcamp 4일차 수업은 4장 Matrix Decomposition을 공부했다. 선형대수의 핵심적 개념인 eigenvalue와 eigenvector가 나오므로 잘 알아두는 게 중요하다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Vector Spaces and Linear Independence</title>
      <link href="https://soonjune.github.io/vector-spaces-and-linear-independence" rel="alternate" type="text/html" title="Vector Spaces and Linear Independence" />
      <published>2021-01-06T20:00:00+09:00</published>
      <updated>2021-01-06T20:00:00+09:00</updated>
      <id>https://soonjune.github.io/vector-spaces-and-linear-independence</id>
      <content type="html" xml:base="https://soonjune.github.io/vector-spaces-and-linear-independence">&lt;p&gt;데이터사이언스대학원 Bootcamp 2일차에는 벡터 공간(Vector space)와 선형 독립(Linear Independence)을 주로 다뤘다. 들어가기에 앞서, 저번 시간에 배웠던 연립 일차 방정식을 풀기 위한 elementary transformation에 대해 잠깐 살펴보자. Elementray row/column operations은 elementary matrix를 우측 또는 좌측에 곱함으로써 수행할 수 있다.(&lt;a href=&quot;https://en.wikipedia.org/wiki/Elementary_matrix#:~:text=From%20Wikipedia%2C%20the%20free%20encyclopedia,when%20R%20is%20a%20field.&quot;&gt;Elementary Matrix&lt;/a&gt;) 기본행연산에는 3가지가 있는데 아래와 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;두 row를 바꾼다 (swap)&lt;/li&gt;
  &lt;li&gt;row에 0이 아닌 상수를 곱한다.&lt;/li&gt;
  &lt;li&gt;두 row를 더한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 세가지 operation은 solution에 영향을 끼치지 않는다. 따라서 이들 연산을 이용해서 행렬을 reduced echelon form(계단 모양으로 나타난다)으로 바꿔 해를 구한다.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;h3 id=&quot;233-the-minus-1-trick&quot;&gt;2.3.3 The Minus-1 Trick&lt;/h3&gt;
&lt;p&gt;행렬 A가 k X n이라면 reduced echeolon form으로 만든 뒤 n X n으로 만들어 주기 위해서 n - k row를 추가한다. 이 때 pivot이 없는 대각선 부분을 -1로 만들어주기 위해 row를 해당 부분에 추가한다. 이 때 -1이 있는 부분의 column을 이용해서 Homogeneous system&lt;sup&gt;&lt;a href=&quot;#footnote_1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;의 바로 해를 찾을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;234-algorithms-for-solving-a-system-of-linear-equations&quot;&gt;2.3.4 Algorithms for Solving a System of Linear Equations&lt;/h3&gt;
&lt;p&gt;Gaussian elimination은 아래 상황에서 모두 사용되기에 매우 유용하다.
(1) determinant를 계산
(2) 선형 독립 여부 확인
(3) 역행렬 연산
(4) rank 계산
(3) 벡터 공간의 basis 구하기&lt;/p&gt;

&lt;p&gt;하지만 효율성을 위해서 실용적으로는 어느 정도의 오차를 감안하고 iterative한 방법이 활용되고 있다.&lt;/p&gt;

&lt;h3 id=&quot;24-vector-spaces&quot;&gt;2.4 Vector Spaces&lt;/h3&gt;
&lt;h4 id=&quot;241-groups&quot;&gt;2.4.1 Groups&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Group_(mathematics)&quot;&gt;Group&lt;/a&gt; 정의.&lt;/p&gt;

&lt;p&gt;n X n 행렬에 대해서도 행렬의 곱셈 정의에 따라 결합법칙이 성립하고 살수 n X n 행렬에 대해 닫혀 있다. 항등원 I가 존재하고 역행렬이 존재할 경우 위에서 정의한 group애 해당한다. 이를 general linear group이라고 부른다.&lt;/p&gt;

&lt;h4 id=&quot;242-vector-spaces&quot;&gt;2.4.2 Vector Spaces&lt;/h4&gt;
&lt;p&gt;V는 집합니다. 이 집합의 원소를 벡터라고 한다. Real-valued vector space V = (V, +, ·)은 아래 두 연산을 갖는 집합이다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;ul&gt;
      &lt;li&gt;: 벡터 덧셈&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;· : 스칼라 곱셈
벡터간 곱셈은 정의되지 않았음에 유의하자.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;243-vector-subspaces&quot;&gt;2.4.3 Vector Subspaces&lt;/h4&gt;
&lt;p&gt;Vector space에 contained된 space. Vector space가 지닌 특성들을 모두 inherit한다. Vector subspace는 closure 조건을 만족시켜야 하며 덧셈에 대한 항등원인 0을 포함해야 한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Vector subspace 간 교집합도 vector subspace이다. (&lt;a href=&quot;https://yutsumura.com/the-intersection-of-two-subspaces-is-also-a-subspace/#:~:text=To%20prove%20that%20the%20intersection,x%E2%88%88U%E2%88%A9V.&quot;&gt;증명&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Ax = O의 solution x가 항상 &lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt;의 subspace이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;25-linear-independence&quot;&gt;2.5 Linear Independence&lt;/h4&gt;
&lt;p&gt;키워드: &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%EA%B2%B0%ED%95%A9&quot;&gt;선형 결합&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_independence#:~:text=In%20the%20theory%20of%20vector,said%20to%20be%20linearly%20independent.&quot;&gt;선형독립&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;선형결합은 Vector space에 있는 유한한 수의 벡터들과 상수배를 이용해서 vector space 내 다른 벡터 v를 나타내는 방법. 선형독립은 선형결합으로 영벡터를 나타내는 방법이 오직 상수(람다)를 모두 0으로 해야만 하는 경우이다.&lt;/p&gt;

&lt;p&gt;Echelon form으로 나타냈을 때 모든 column이 pivot column일 경우에만 선형독립이다(필요충분)&lt;/p&gt;

&lt;h3 id=&quot;26-basis-and-rank&quot;&gt;2.6 Basis and Rank&lt;/h3&gt;
&lt;h4 id=&quot;261-generating-set-and-basis&quot;&gt;2.6.1 Generating Set and Basis&lt;/h4&gt;
&lt;p&gt;Generating set은 Vector Space V에 속하는 벡터 v가 유한(k)개의 벡터로 나타낼 수 있으면 이 벡터들의 집합 A를 generating set이라고 한다. Generating set A를 가지고 만들 수 있는 모든 선형결합의 집합을 span of A라고 정의한다.&lt;/p&gt;

&lt;p&gt;Basis는 Vector space V의 가장 작은 generating set이다. 가장 작다는 것은 해당 generating set보다 더 작은 generating set의 조건을 만족하는 집합이 없음을 의미한다. Generating set의 선형독립인 vector들을 V의 basis라고 부른다.&lt;/p&gt;

&lt;p&gt;Basis는 unique하지 않지만 벡터 수는 같다. 이때 vector space의 차원은 basis 벡터의 수와 같다. basis를 이용해서 vector space 내 다른 vector를 표현할 때의 해는 유일하다.&lt;/p&gt;

&lt;h4 id=&quot;262-rank&quot;&gt;2.6.2 Rank&lt;/h4&gt;
&lt;p&gt;행렬 내에 independent한 행 또는 열의 수를 rank라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;27-linear-mappings&quot;&gt;2.7 Linear Mappings&lt;/h3&gt;
&lt;p&gt;벡터 공간의 성질을 만족시키기 위해서 mapping할 때 특정 성질을 만족하는 경우 linear mapping이라고 한다. Linear mapping을 행렬로 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;Image와 Kernel(또는 Null Space)은 다음과 같이 정의한다. Kernel은 영행렬이 나오게끔하는 정의역 내 벡터들의 집합. Image는 치역으로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&quot;https://brilliant.org/wiki/rank-nullity-theorem/#:~:text=The%20rank%2Dnullity%20theorem%20states,columns%20over%20a%20field%2C%20then&quot;&gt;Rank-Nullity Theorem&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;footnote_1&quot;&gt;1&lt;/a&gt;: A system of linear equations is homogeneous if all of the constant terms are zero&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Park Seung Joon</name>
        
        
      </author>

      

      
        <category term="linear-algebra" />
      
        <category term="math" />
      

      
        <summary type="html">데이터사이언스대학원 Bootcamp 2일차에는 벡터 공간(Vector space)와 선형 독립(Linear Independence)을 주로 다뤘다. 들어가기에 앞서, 저번 시간에 배웠던 연립 일차 방정식을 풀기 위한 elementary transformation에 대해 잠깐 살펴보자. Elementray row/column operations은 elementary matrix를 우측 또는 좌측에 곱함으로써 수행할 수 있다.(Elementary Matrix) 기본행연산에는 3가지가 있는데 아래와 같다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Mathematics for Machine Learning</title>
      <link href="https://soonjune.github.io/Linear-Algebra" rel="alternate" type="text/html" title="Mathematics for Machine Learning" />
      <published>2021-01-05T20:00:00+09:00</published>
      <updated>2021-01-05T20:00:00+09:00</updated>
      <id>https://soonjune.github.io/Linear-Algebra</id>
      <content type="html" xml:base="https://soonjune.github.io/Linear-Algebra">&lt;p&gt;서울대 데이터사이언스대학원 Bootcamp를 수강하면서 배운 내용을 정리해보고자 한다. 1주차에는 데이터사이언스를 위한 수학을 주제로 오민환 교수님께서 강의를 진행했다. 교재는 &lt;a href=&quot;https://mml-book.github.io/book/mml-book.pdf&quot;&gt;Mathematics for Machine Learning&lt;/a&gt;이라는 오픈 소스 교재를 사용하였다. 오늘은 Ch2까지 배운 선형대수의 기초적인 내용을 정리해본다.&lt;/p&gt;

&lt;h3 id=&quot;i-책-소개-intro&quot;&gt;I. 책 소개 (Intro)&lt;/h3&gt;
&lt;p&gt;교재에서는 머신러닝을 위한 기초적인 지식으로 3가지롤 꼽고 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;프로그래밍 언어 및 데이터 분석 도구&lt;/li&gt;
  &lt;li&gt;대규모 컴퓨팅 및 그와 관련된 프레임워크&lt;/li&gt;
  &lt;li&gt;수학, 통계 지식
이 책에서는 마지막 부분, 즉 수학과 연관된 부분을 다룬다. 모델 자체의 알고리즘에 초점을 맞추기보다는 수학적인 개념과 실용적인 궁금증을 해결하고자 노력했다고 저자는 밝히고 있다.
    &lt;h4 id=&quot;핵심-단어-데이터data-모델model-학습learning&quot;&gt;핵심 단어: 데이터(data), 모델(model), 학습(learning)&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;h3 id=&quot;ii-선형대수-linear-algebra&quot;&gt;II. 선형대수 (Linear Algebra)&lt;/h3&gt;
&lt;p&gt;(1) Geometric vectors / Polynomials =&amp;gt; They are both vectors. Audio signals, elements of &lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt; are vectors.
(2) Closure(닫힘) - 그 집합의 원소와 관계가 있는 원소가 항상 그 집합에 속한다는 성질이다.
&lt;img src=&quot;https://miro.medium.com/max/4800/1*5haUfmOWQUh9N353hMy9KQ.png&quot; alt=&quot;mind-map&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;21-연립-일차-방정식-systems-of-linear-equations&quot;&gt;2.1 연립 일차 방정식 (Systems of Linear Equations)&lt;/h3&gt;
&lt;p&gt;해는 없거나 / 하나만 존재하거나 / 무수히 많을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;22-행렬-matrices&quot;&gt;2.2 행렬 (Matrices)&lt;/h3&gt;
&lt;p&gt;연립 일차 방정식을 행렬을 이용해서 간결하게 표현할 수 있다. &lt;br /&gt;
행렬은 또한 선형 변환(Linear mapping)이라는 함수를 표현할 수 있다. &lt;br /&gt;
행렬의 덧셈과 곱셈 정의. 곱셈에 대하여 교환법칙이 성립하지 않는다.&lt;/p&gt;

&lt;p&gt;cf . 아인슈타인 표기법
np.einsum을 이용해서 나타낼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;222-역행렬과-전치행렬-inverse-and-transpose&quot;&gt;2.2.2 역행렬과 전치행렬 (Inverse and Transpose)&lt;/h3&gt;
&lt;p&gt;행렬 A의 역행렬이 존재하는 경우 regular/invertible/nonsingular라고 부름. &lt;br /&gt;
역행렬이 존재하면 고유(unique)하다.&lt;/p&gt;

&lt;p&gt;Theorem 증명&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(Uniqueness of Inverse Matrix) If B and C are both inverses of an n × n matrix A, then B = C.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;B = B I_n = B(A C) = (B A)C =I_nC = C.&lt;/script&gt;

&lt;p&gt;결합 법칙을 이용해서 증명이 가능하다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;AB = I&lt;/script&gt; then &lt;script type=&quot;math/tex&quot;&gt;BA = I&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A, B를 n 정사각행렬이라 하고 n-dimensional 벡터 공간을 정의하면 AB의 range는 full space. 따라서 B의 range도 dimension n이다. &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;𝐵=𝐵𝐼=𝐵(𝐴𝐵)=(𝐵𝐴)𝐵&lt;/script&gt;. By the distributive law, &lt;script type=&quot;math/tex&quot;&gt;(𝐼−𝐵𝐴)𝐵=0&lt;/script&gt;. B는 full range이므로 I-BA가 영행렬이 되어야 한다. 따라서 I = BA.&lt;/p&gt;

&lt;p&gt;=&amp;gt; full rank이면 invertible하다. &lt;a href=&quot;https://sharmaeklavya2.github.io/theoremdep/nodes/linear-algebra/matrices/full-rank-inv.html&quot;&gt;증명&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이외에도 augmented matrix를 Gaussian eliminiation을 통해서 Row-Echelon Form으로 변환한 뒤 연랍 일차 방정식을 푸는 방법에 대해서 공부하였다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Park Seung Joon</name>
        
        
      </author>

      

      
        <category term="linear-algebra" />
      
        <category term="math" />
      

      
        <summary type="html">서울대 데이터사이언스대학원 Bootcamp를 수강하면서 배운 내용을 정리해보고자 한다. 1주차에는 데이터사이언스를 위한 수학을 주제로 오민환 교수님께서 강의를 진행했다. 교재는 Mathematics for Machine Learning이라는 오픈 소스 교재를 사용하였다. 오늘은 Ch2까지 배운 선형대수의 기초적인 내용을 정리해본다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">4. Subspace</title>
      <link href="https://soonjune.github.io/Subspace" rel="alternate" type="text/html" title="4. Subspace" />
      <published>2020-10-10T22:00:00+09:00</published>
      <updated>2020-10-10T22:00:00+09:00</updated>
      <id>https://soonjune.github.io/Subspace</id>
      <content type="html" xml:base="https://soonjune.github.io/Subspace">&lt;p&gt;오늘은 mit Gilbert Strang 선생님의 선형대수 Subspace에 관한 부분을 정리해 보았다.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;h3 id=&quot;i-4-fundamental-subspaces&quot;&gt;I. 4 Fundamental Subspaces&lt;/h3&gt;
&lt;p&gt;A is m x n&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;columnspace C(A) in &lt;script type=&quot;math/tex&quot;&gt;R^m&lt;/script&gt;  &lt;br /&gt;
dimension of C(A) = # of pivot columns = rank r&lt;/li&gt;
  &lt;li&gt;nullspace N(A) in &lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt;  &lt;br /&gt;
dimension of N(A) = # of free variables = n - r&lt;/li&gt;
  &lt;li&gt;rowspace = all combs of rows = all combs of columns = &lt;script type=&quot;math/tex&quot;&gt;A^T = C(A^T)&lt;/script&gt;  &lt;br /&gt;
same dimension as columnspace&lt;/li&gt;
  &lt;li&gt;nullspace of &lt;script type=&quot;math/tex&quot;&gt;A^T&lt;/script&gt; N(&lt;script type=&quot;math/tex&quot;&gt;A^T&lt;/script&gt;) = left nullspace of A &lt;br /&gt;
dimension = m - r
left nullspace라는 이름은 row벡터로 만들면 왼쪽에 오기 때문 (&lt;script type=&quot;math/tex&quot;&gt;y^TA = O&lt;/script&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ii-orthogonal-vectors-and-subspaces&quot;&gt;II. Orthogonal Vectors and Subspaces&lt;/h3&gt;
&lt;p&gt;1) Test for orthogonality: dot product is zero(&lt;script type=&quot;math/tex&quot;&gt;X^Ty = 0&lt;/script&gt;) &lt;br /&gt;
orthogonal한 벡터 간에는 피타고라스 정리가 성립한다. &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;X^TX + Y^TY = (X+Y)^T(X+Y)&lt;/script&gt; &lt;br /&gt;
2) Subspace S와 T가 orthogonal하다는 것의 의미 &lt;br /&gt;
S에 속한 모든 벡터가 T에 속한 모든 벡터와 orthogonal하다. &lt;br /&gt;
=&amp;gt; Rowspace is Orthogonal to nullspace - 이유
&lt;script type=&quot;math/tex&quot;&gt;Ax = O&lt;/script&gt; &lt;br /&gt;
=&amp;gt; Rowspace and Nullspace are orthogonal complements in &lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt; // 둘의 dimension을 더하면 n 
Nullspace contains all vectors perpendicular to rowspace&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Ax = b&lt;/script&gt; =&amp;gt; 양쪽변에 &lt;script type=&quot;math/tex&quot;&gt;A^T&lt;/script&gt;를 곱해준다.
How to “solve” the equation when there is no solution?  &lt;br /&gt;
m &amp;gt; n&lt;br /&gt;
실생활에서 noise 때문에 A를 제대로 알 수 없는 경우 - noise를 제거해야.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt;: square, symmetric, invertible?
&lt;script type=&quot;math/tex&quot;&gt;N(A^TA) = N(A)&lt;/script&gt; &lt;br /&gt;
rank of &lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; = rank of A &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;A^TA&lt;/script&gt; is invertible exactly if A has independent columns&lt;/p&gt;

&lt;h3 id=&quot;iii-projections-onto-subspaces&quot;&gt;III. Projections onto Subspaces&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Projection Matrix (P) &lt;br /&gt;
projp = Pb   P = &lt;script type=&quot;math/tex&quot;&gt;(aa^T) / (a^Ta)&lt;/script&gt; &lt;br /&gt;
columnspace of C(P) = line through a &lt;br /&gt;
rank(P) = 1 &lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;P is symmetric&lt;/li&gt;
      &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P^2 = P&lt;/script&gt; &lt;br /&gt;
Projection이 필요한 이유? Ax=b의 해가 없을 수도 있어서
가장 비슷한 문제를 푼다. =&amp;gt; Solve Ax = p(proj of b onto columnspace)
Key: b-Ax is perpendicular to plane
&lt;script type=&quot;math/tex&quot;&gt;(a_1)^T(b-Ax)=0, (a_2)^T(b-Ax)=0&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;A^T(b-Ax) = O&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;What subsapce is error vector e(b-Ax) in?&lt;br /&gt;
e is in &lt;script type=&quot;math/tex&quot;&gt;N(A^T)&lt;/script&gt;  =&amp;gt;
e is perpendicular to C(A) &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;A^TAx = A^Tb&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;x = (A^TA)^(-1)A^Tb&lt;/script&gt;
P = Ax = A(A^TA)^(-1)A^Tb&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P^T = P&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;P^2 = P&lt;/script&gt; &lt;br /&gt;
Application: Least squares fitting by a line &lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Park Seung Joon</name>
        
        
      </author>

      

      
        <category term="linear-algebra" />
      
        <category term="math" />
      

      
        <summary type="html">오늘은 mit Gilbert Strang 선생님의 선형대수 Subspace에 관한 부분을 정리해 보았다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">3. Determinant</title>
      <link href="https://soonjune.github.io/Determinants" rel="alternate" type="text/html" title="3. Determinant" />
      <published>2020-10-04T22:00:00+09:00</published>
      <updated>2020-10-04T22:00:00+09:00</updated>
      <id>https://soonjune.github.io/Determinants</id>
      <content type="html" xml:base="https://soonjune.github.io/Determinants">&lt;p&gt;오랜만에 선형대수 정리를 다시 시작한다. 오늘은 판별식(Determinant)에 대해 알아보자.
판별식은 고유값(eigenvalue)을 구하기 위해 필요한 식 중 하나다.
유명한 &lt;a href=&quot;https://www.youtube.com/watch?v=srxexLishgY&quot;&gt;Gilbert Strang 선생님의 MIT 강의&lt;/a&gt;를 나만의 말로 정리해 보았다.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;h3 id=&quot;판별식의-성질&quot;&gt;판별식의 성질&lt;/h3&gt;
&lt;p&gt;1) det I = 1 &lt;br /&gt;
2) 두 행을 바꾼 행렬의 판별식은 부호가 반대다. &lt;br /&gt;
3a) 한 행에 t를 곱하면 전체에 본래 판별식에 t를 곱한 것과 같다. &lt;br /&gt;
3b) 한 행에만 임의 값을 더해주면 본래 판별식에 더해주는 값을 분리한 행렬(나머지 행의 값은 동일)의 판별식의 합과 같다. &lt;br /&gt;
Determinant is Linear Each Row &lt;br /&gt;
4) 두 행의 값이 같으면 =&amp;gt; 판별식은 0이다.(2로부터 유도) &lt;br /&gt;
5) 한 행을 곱해서 다른 행에서 빼도 판별식이 달라지지 않는다.(3b, 3a, 4로부터 유도) &lt;br /&gt;
6) 한 행의 원소가 모두 0이라면 판별식도 0이다.(3a 또는 3b로 유도) &lt;br /&gt;
7) 삼각행렬의 판별식은 대각선에 위치한 원소들의 곱이다. (소프트웨어에서 판별식을 구하는 방식 -&amp;gt; 삼각행렬도 만들어서 대각선 곱한다. 3a를 이용해서 행마다 factor out하면 대각선 곱과 1이 남는다.)&lt;br /&gt;
8) 행렬의 역행렬이 존재하지 않으면 판별식은 0이다. &lt;br /&gt;
9) det AB = (det A) * (det B) &lt;br /&gt;
10) &lt;script type=&quot;math/tex&quot;&gt;detA^T = detA&lt;/script&gt; =&amp;gt; 열이 다 0이어도 판별식은 0이다를 보일 수 있다. (증명은 &lt;script type=&quot;math/tex&quot;&gt;|U^T||L^T| = |L||U|&lt;/script&gt; 보임으로써 할 수 있다.) &lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;판별식-공식&quot;&gt;판별식 공식&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/b5300123cfe333f7a1e78d8dab5eb044d0be1548&quot; /&gt;
소행렬식(Minor) &lt;script type=&quot;math/tex&quot;&gt;M_(ij)&lt;/script&gt;는 i행과 j열을 제외한 행렬식이다.
아래서 보듯, cofactor는 i+j가 짝수이면 양수 i+j가 홀수이면 음수를 앞에 곱한다.
&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/279a8d3f0f8f1338b8d2a4f80f1323f514edf60d&quot; /&gt;
&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/089ce40623558856f6cd09a7c0dc23fded7d1e08&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cofactor를-이용한-역행렬-구하기&quot;&gt;Cofactor를 이용한 역행렬 구하기&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/0651000d4523a0563cd016ae064f75011e0d8702&quot; /&gt;
&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/5b076e517a662a54fe32bfcd7ca2ec8a1998b139&quot; /&gt;
&lt;script type=&quot;math/tex&quot;&gt;AC^T = (detA)I&lt;/script&gt;임을 보이면 된다. 행렬곱의 대각선은 detA가 되고 나머지는 0이 된다는 것을 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;cramers-rule&quot;&gt;Cramer’s Rule&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/4b063cb0d9acc0aed7f58ad881d845a2e2598e7f&quot; /&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Park Seung Joon</name>
        
        
      </author>

      

      
        <category term="linear-algebra" />
      
        <category term="math" />
      

      
        <summary type="html">오랜만에 선형대수 정리를 다시 시작한다. 오늘은 판별식(Determinant)에 대해 알아보자. 판별식은 고유값(eigenvalue)을 구하기 위해 필요한 식 중 하나다. 유명한 Gilbert Strang 선생님의 MIT 강의를 나만의 말로 정리해 보았다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">2. Matrix Algebra</title>
      <link href="https://soonjune.github.io/Matrix-Algebra-1" rel="alternate" type="text/html" title="2. Matrix Algebra" />
      <published>2020-08-11T22:00:00+09:00</published>
      <updated>2020-08-11T22:00:00+09:00</updated>
      <id>https://soonjune.github.io/Matrix-Algebra-1</id>
      <content type="html" xml:base="https://soonjune.github.io/Matrix-Algebra-1">&lt;p&gt;선형대수 공부 3일차 - 행렬에 대해 알아보자&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;h2 id=&quot;21-matrix-operations&quot;&gt;2.1 Matrix Operations&lt;/h2&gt;
&lt;h3 id=&quot;theorem-1&quot;&gt;Theorem 1&lt;/h3&gt;
&lt;p&gt;덧셈, 뺄셈, scalar multiple은 교환, 결합, 분배 법칙이 성립한다.&lt;/p&gt;

&lt;p&gt;행렬의 곱셈은 조금 다른데, linear transformation, function으로 이해할 수 있다.  &lt;br /&gt;
따라서 곱할 수록 span을 작아진다. 즉, span(A)가 span(AB)를 포함한다.&lt;/p&gt;
&lt;h3 id=&quot;theorem-2&quot;&gt;Theorem 2&lt;/h3&gt;
&lt;p&gt;곱셈은 결합법칙, 오른쪽/왼쪽 분배 법칙이 성립하나 교환법칙은 성립하지 않는다. 즉, AB와 BA는 같지 않을 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Powers of a Matrix  &lt;br /&gt;
A가 &lt;script type=&quot;math/tex&quot;&gt;nXn&lt;/script&gt; 행렬이라면 &lt;script type=&quot;math/tex&quot;&gt;A^n&lt;/script&gt;과 같이 행렬의 거듭제곱을 정의할 수 있다. &lt;script type=&quot;math/tex&quot;&gt;A^0 = I&lt;/script&gt;로 정의한다.&lt;/li&gt;
  &lt;li&gt;Transpose  &lt;br /&gt;
행렬의 열과 행을 뒤바꾼 것이라 보면 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;a. &lt;script type=&quot;math/tex&quot;&gt;(A^T)^T = A&lt;/script&gt;    &lt;br /&gt;
b. &lt;script type=&quot;math/tex&quot;&gt;(A + B)^T = A^T + B^T&lt;/script&gt;   &lt;br /&gt;
c. For any scalar &lt;script type=&quot;math/tex&quot;&gt;r, (rA)^T = rA^T&lt;/script&gt;   &lt;br /&gt;
d. &lt;script type=&quot;math/tex&quot;&gt;(AB)^T = B^TA^T&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;22-the-inverse-of-a-matrix&quot;&gt;2.2 The Inverse of a Matrix&lt;/h2&gt;
&lt;h3 id=&quot;theorem-4&quot;&gt;Theorem 4&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Invertible_matrix#Inversion_of_2_%C3%97_2_matrices&quot;&gt;2X2 행렬의 역행렬 공식&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;theorem-5&quot;&gt;Theorem 5&lt;/h3&gt;
&lt;p&gt;Ax = b이고 A가 역행렬이 존재하면 하나의 유일해 &lt;script type=&quot;math/tex&quot;&gt;x=A^{-1}b&lt;/script&gt;를 지닌다.&lt;/p&gt;

&lt;p&gt;참고) at least 1 solution &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;=&gt; %]]&gt;&lt;/script&gt; no free variable&lt;/p&gt;
&lt;h3 id=&quot;theorem-6&quot;&gt;Theorem 6&lt;/h3&gt;
&lt;p&gt;a. &lt;script type=&quot;math/tex&quot;&gt;(A^{-1})^{-1} = A&lt;/script&gt;   &lt;br /&gt;
b. &lt;script type=&quot;math/tex&quot;&gt;(AB)^{-1} = B^{-1}A{-1}&lt;/script&gt;   &lt;br /&gt;
c. &lt;script type=&quot;math/tex&quot;&gt;(A^T)^{-1} = (A^{-1})^T&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;theorem-7&quot;&gt;Theorem 7&lt;/h3&gt;
&lt;p&gt;nXn 행렬이 &lt;script type=&quot;math/tex&quot;&gt;I_n&lt;/script&gt;과 row equivalent, 즉 echelon row reduction을 통해 &lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt;로 만들 수 있다면 역행렬이 존재한다.  &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;[A|I]&lt;/script&gt; -&amp;gt; &lt;script type=&quot;math/tex&quot;&gt;[I|C]&lt;/script&gt;로 row reduction을 거치면 C가 역행렬이다.&lt;/p&gt;

&lt;h2 id=&quot;23-characterizations-of-invertible-matrices&quot;&gt;2.3 Characterizations of Invertible Matrices&lt;/h2&gt;
&lt;h3 id=&quot;theorem-8&quot;&gt;Theorem 8&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/YUK4bQv.png&quot; align=&quot;center&quot; sytle=&quot;height: 100px;&quot; /&gt;&lt;/p&gt;
&lt;div&gt;
&lt;img src=&quot;https://i.imgur.com/ulOSO1I.png&quot; align=&quot;left&quot; /&gt;
&lt;img src=&quot;https://i.imgur.com/AwTwn4T.png&quot; align=&quot;right&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;위와 같은 조건을 하나만 만족해도 역행렬이 존재하고 하나만 위배해도 역행렬이 존재하지 않는다. 마치 고리와 같이 상호 연결되어 있고 증명과정도 하나 하나 연결하면서 확인할 수 있다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Park Seung Joon</name>
        
        
      </author>

      

      
        <category term="linear-algebra" />
      
        <category term="math" />
      

      
        <summary type="html">선형대수 공부 3일차 - 행렬에 대해 알아보자</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">1. Linear Equations in Linear Algebra (2)</title>
      <link href="https://soonjune.github.io/Linear-Equations-in-Linear-Algebra-2" rel="alternate" type="text/html" title="1. Linear Equations in Linear Algebra (2)" />
      <published>2020-08-09T21:00:00+09:00</published>
      <updated>2020-08-09T21:00:00+09:00</updated>
      <id>https://soonjune.github.io/Linear-Equations-in-Linear-Algebra-2</id>
      <content type="html" xml:base="https://soonjune.github.io/Linear-Equations-in-Linear-Algebra-2">&lt;p&gt;선형대수 공부 2일차 - 오늘 배운 내용을 간단히 정리해보자&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;h2 id=&quot;15-solution-sets-of-linear-systems&quot;&gt;1.5 Solution Sets of Linear Systems&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;homogeneous: A linear equation is called homogeneous if its constant term is zero. Such an equation has the form&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;text-align: center; margin: 0; padding: 0;&quot;&gt; $$a_1x_1 + a_2x_2 + ... + a_nx_n = 0$$ &lt;/p&gt;
&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;a_1, a_2, . . . , a_n&lt;/script&gt; are constants and &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2, . . . , x_n&lt;/script&gt; are variables.
A homogeneous linear system is a linear system whose equations are all homogeneous. For example,&lt;/p&gt;
&lt;p style=&quot;text-align: center; margin: 0; padding: 0;&quot;&gt;
$$3x_1 − 9x_2 − 9x_3 − 6x_4 = 0$$
$$2x_1 − 8x_2 − 6x_3 + 2x_4 = 0$$
$$−2x_1 + 3x_2 + 8x_3 + 7x_4 = 0$$
&lt;/p&gt;

&lt;p&gt;is a homogeneous linear system.[1]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;즉, 행렬을 사용해서 Ax = 0 꼴로 쓸 수 있으면 최소 하나의 해를 x가 영벡터인 최소 하나의 해를 가지며 이를 trivial solution이라 한다.  &lt;br /&gt;
non-trivial한 해를 갖기 위해서는 방정식이 최소 하나의 free variable을 가져야한다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;16-applications-of-linear-systems&quot;&gt;1.6 Applications of Linear Systems&lt;/h2&gt;
&lt;p&gt;경제, network flow 등 다양하게 활용이 가능하다.&lt;/p&gt;

&lt;h2 id=&quot;17-linear-independence&quot;&gt;1.7 Linear Independence&lt;/h2&gt;
&lt;p&gt;선형 독립이라는 말이 나오며 앞서 본 방정식이 trivial solution을 가질 때 독립이라 한다.
벡터의 해가 되는 계수들이 모두 0이어야 하며 그렇지 않은 경우가 하나라도 있으면 linearly dependent하다.&lt;/p&gt;
&lt;h3 id=&quot;theorem-7-characterization-of-linearly-dependent-sets&quot;&gt;Theorem 7: Characterization of Linearly Dependent Sets&lt;/h3&gt;
&lt;p&gt;집합 S에 속한 벡터들 중 하나라도 다른 벡터의 선형 결합으로 표현할 수 있다면 linearly dependent하다.  &lt;br /&gt;
벡터를 하나씩 추가해 가면서 linearly dependent한지 여부를 확인할 수 있다. &lt;br /&gt;
추가된 벡터를 앞선 벡터들의 linear combination으로 표현할 수 있는지 확인하면 된다.&lt;/p&gt;
&lt;h3 id=&quot;theorem-8&quot;&gt;Theorem 8&lt;/h3&gt;
&lt;p&gt;If a set contains more vectors than there are entries in each vector, then the set is linearly dependent. That is, any set &lt;script type=&quot;math/tex&quot;&gt;{v_1,...,v_p}&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt; is linearly dependent if p &amp;gt; n.
즉 행렬의 열의 개수가 행의 개수보다 많다면 linearly dependent하다고 할 수 있다.  &lt;br /&gt;
그 반대 p &amp;lt; n인 경우는 상황에 따라 다르기 때문에 일반화할 수 없다.&lt;/p&gt;
&lt;h3 id=&quot;theorem-9&quot;&gt;Theorem 9&lt;/h3&gt;
&lt;p&gt;If a set S D &lt;script type=&quot;math/tex&quot;&gt;{v_1,..., v_p}&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt; contains the zero vector, then the set is linearly dependent.   &lt;br /&gt;
Zero vector가 존재하면 linearly dependent하다.&lt;/p&gt;

&lt;h2 id=&quot;18-introduction-to-linear-transformations&quot;&gt;1.8 Introduction to Linear Transformations&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_map&quot;&gt;transformation&lt;/a&gt;은 function, mapping과 같은 말이다.
&lt;script type=&quot;math/tex&quot;&gt;T: R^n -&gt; R^m&lt;/script&gt;인 linear transformation에 대하여&lt;/li&gt;
  &lt;li&gt;domain(정의역) - &lt;script type=&quot;math/tex&quot;&gt;R^n&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;codomain(공역) - &lt;script type=&quot;math/tex&quot;&gt;R^m&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;range(치역) / image - T(x)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;로 정의한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear Transformation은 아래 두 조건을 만족해야 한다.  &lt;br /&gt;
i. T(u+v) = T(u) + T(v)    &lt;br /&gt;
ii. T(cu) = cT(u) for all scalars c   &lt;br /&gt;
위 조건으로부터 T(0) = 0이 유도된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;오늘 정리로 챕터1을 마쳤다. 중간고사도 여기까지가 범위인 것 같다. 이틀만에 중간고사 범위까지 공부해서 기분이 좋다(?).&lt;/p&gt;

&lt;p&gt;[1] 출처: http://faculty.bard.edu/~belk/math213f15/Notes10.pdf&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Park Seung Joon</name>
        
        
      </author>

      

      
        <category term="linear-algebra" />
      
        <category term="math" />
      

      
        <summary type="html">선형대수 공부 2일차 - 오늘 배운 내용을 간단히 정리해보자</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">1. Linear Equations in Linear Algebra (1)</title>
      <link href="https://soonjune.github.io/Linear-Equations-in-Linear-Algebra-1" rel="alternate" type="text/html" title="1. Linear Equations in Linear Algebra (1)" />
      <published>2020-08-08T22:00:00+09:00</published>
      <updated>2020-08-08T22:00:00+09:00</updated>
      <id>https://soonjune.github.io/Linear-Equations-in-Linear-Algebra-1</id>
      <content type="html" xml:base="https://soonjune.github.io/Linear-Equations-in-Linear-Algebra-1">&lt;p&gt;오늘 선형대수 공부를 시작했다. 자료는 고려대에 계시다가 현재는 KAIST AI 대학원으로 가신 주재걸 교수님의 &lt;a href=&quot;https://www.youtube.com/watch?v=3bTr91fLHOg&amp;amp;list=PLep-kTP3NkcOBWLIfjMYymsdagnGi3XS6&quot;&gt;강의&lt;/a&gt;와 수업에 사용한 &lt;a href=&quot;https://www.amazon.com/Linear-Algebra-Its-Applications-5th/dp/032198238X&quot;&gt;교과서&lt;/a&gt;를 참고했다.
간단히 오늘 배운 내용을 정리해보자.&lt;/p&gt;

&lt;p&gt;오늘은 간단한 개요 정도였다. 요즘 교육과정에서는 행렬이 빠졌다고 하는데 구7차 교육과정에서 처음으로 등장하는 것이 행렬이었던 것 같다. 그러면 섹션별로 간단한 개념만 짚고 넘어가자.&lt;/p&gt;

&lt;h2 id=&quot;11-systems-of-linear-equations&quot;&gt;1.1 Systems of Linear Equations&lt;/h2&gt;
&lt;p&gt;1) &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_equation&quot;&gt;linear equation의 정의&lt;/a&gt; &lt;br /&gt;
=&amp;gt; 이러한 linear equation(s)의 집합을 &lt;a href=&quot;https://en.wikipedia.org/wiki/System_of_linear_equations&quot;&gt;systems of linear equations&lt;/a&gt; 라 한다.   &lt;br /&gt;
2) &lt;a href=&quot;https://en.wikipedia.org/wiki/Consistent_and_inconsistent_equations&quot;&gt;consistent&lt;/a&gt; - 하나 이상의 해가 존재하는 경우 / 그 반대로 해가 없는 경우는 inconsistent   &lt;br /&gt;
3) 선형방정식을 행렬로 나타낼 수 있다. (coefficient matrix, augmented matrix)   &lt;br /&gt;
4) linear system을 푸는 가장 기본적인 방법은 triangular form으로 만들어 주는 것이다. 이때 아래의 3가지 기본 row operation을 사용한다.&lt;br /&gt;
=&amp;gt; Replacement, Interchange, Scaling &lt;br /&gt;
세가지 row operation을 통해 만들 수 있는 행렬들은 row equivalent하다고 표현한다.&lt;/p&gt;

&lt;h2 id=&quot;12-row-reduction-and-echelon-forms&quot;&gt;1.2 Row Reduction and Echelon Forms&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/3743aca294b2e5346c167819fd9ee0bcb79ef22c&quot; align=&quot;left&quot; /&gt;    
&lt;/p&gt;

&lt;p&gt;위와 같은 모양을 지닌 행렬의 형태를 &lt;a href=&quot;https://en.wikipedia.org/wiki/Row_echelon_form&quot;&gt;echelon form&lt;/a&gt; 이라고 한다.  &lt;br /&gt;
여기서 각 행의 맨 앞의 0이 아닌 수를 leading entry라고 한다. (여기서는 1, 2, 1)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/0cbe2a26712d3e37787d46b087e97748740b553e&quot; align=&quot;left&quot; /&gt;
&lt;/p&gt;
&lt;p&gt;위와 같이 leading entry가 1이고 leading entry 밑뿐만 아니라 위의 원소들도 0이면 reduced echelon form이라고 부른다.&lt;/p&gt;

&lt;h3 id=&quot;theorem-1-uniqueness-of-the-reduced-echelon-form&quot;&gt;Theorem 1: Uniqueness of the Reduced Echelon Form&lt;/h3&gt;
&lt;p&gt;우선, 모든 행렬은 (reduced) echelon form으로 변환할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Echelon form인 행렬의 leading entry를 pivot position이라고 한다.  &lt;br /&gt;
각 행에 pivot position이 존재하느냐의 여부에 따라이 해당 열이 basic variable인지 free variable인지 결정된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;theorem-2-existence-and-uniqueness-theorem&quot;&gt;Theorem 2: Existence and Uniqueness Theorem&lt;/h3&gt;
&lt;p&gt;Echelon form의 행렬에서 [0, …, b] 꼴로 나오는 행(b는 0이 아닌 실수)이 존재하면 해가 없다.&lt;/p&gt;

&lt;h2 id=&quot;13-vector-equations&quot;&gt;1.3 Vector Equations&lt;/h2&gt;
&lt;p&gt;교환 법칙, 결합 법칙 등이 성립한다. &lt;br /&gt;
span의 개념을 확인하자 - set of all possible linear combinations   &lt;br /&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=k7RM-ot2NWY&quot;&gt;등장개념 영상&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;14-the-matrix-equation-ax--b&quot;&gt;1.4 The Matrix Equation Ax = b&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/1mgvPjG.png&quot; alt=&quot;theorem 3 and 4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;공부를 하면서 최근에 본 물리학자인 &lt;a href=&quot;https://youtu.be/8n80LX2LGjY?t=90&quot;&gt;파인만의 영상&lt;/a&gt;이 떠올랐다. 선형대수도 직관이 더해지면 더 오래 기억하고 의미있게 활용할 수 있을 것이다. 단순히 해를 구하는 것은 컴퓨터가 더 잘하니 숨겨진 의의를 찾는데 집중하면 좋을 것 같다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Park Seung Joon</name>
        
        
      </author>

      

      
        <category term="linear-algebra" />
      
        <category term="math" />
      

      
        <summary type="html">오늘 선형대수 공부를 시작했다. 자료는 고려대에 계시다가 현재는 KAIST AI 대학원으로 가신 주재걸 교수님의 강의와 수업에 사용한 교과서를 참고했다. 간단히 오늘 배운 내용을 정리해보자.</summary>
      

      
      
    </entry>
  
</feed>
